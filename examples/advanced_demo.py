#!/usr/bin/env python3
"""
Advanced ClimFinRisk Platform Demonstration

This script demonstrates the groundbreaking innovations that make this platform
novel and meaningful for physical climate risk modeling.

Principal Investigator: Dr. Ian Helfrich
Institution: Georgia Institute of Technology
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path

from climfinrisk import ClimateRiskModeler
from climfinrisk.data import DataIngestion, DataPreprocessor
from climfinrisk.modeling import DimensionalityReduction, RiskEstimation, VulnerabilityCurves
from climfinrisk.geospatial import SpatialAnalyzer
from climfinrisk.utils import ConfigManager, DataValidator, Logger

def main():
    """
    Demonstrate the advanced capabilities of the ClimFinRisk platform.
    """
    print("ğŸŒ ClimFinRisk Platform - Advanced Demonstration")
    print("=" * 60)
    print("Principal Investigator: Dr. Ian Helfrich")
    print("Institution: Georgia Institute of Technology")
    print("=" * 60)
    
    config = ConfigManager()
    logger = Logger(config.get('logging', {}))
    
    print("\nğŸš€ GROUNDBREAKING INNOVATIONS DEMONSTRATED:")
    print("1. Multi-Dimensional Tensor Decomposition for Climate Risk")
    print("2. Adaptive Vulnerability Curves with Bayesian Updating")
    print("3. Real-Time Risk Propagation Networks")
    print("4. Advanced Uncertainty Quantification")
    print("5. Portable Modular Architecture")
    
    print("\n" + "="*50)
    print("ğŸ”¬ DEMO 1: Multi-Dimensional Tensor Decomposition")
    print("="*50)
    
    data_ingestion = DataIngestion(config.get('data', {}))
    dim_reducer = DimensionalityReduction(config.get('modeling', {}))
    
    bbox = {
        'min_lat': 25.0, 'max_lat': 50.0,
        'min_lon': -125.0, 'max_lon': -65.0  # Continental US
    }
    
    print("ğŸ“Š Loading multi-hazard climate data...")
    climate_data = data_ingestion.load_climate_data(
        hazard_types=['flood', 'cyclone', 'drought'],
        scenarios=['rcp26', 'rcp45', 'rcp85'],
        bbox=bbox
    )
    
    print(f"âœ… Climate data loaded: {dict(climate_data.dims)}")
    print(f"ğŸŒªï¸ Variables: {list(climate_data.data_vars.keys())}")
    
    print("\nğŸ”¬ Applying Tensor SVD for Multi-Hazard Analysis...")
    hazard_patterns = dim_reducer.extract_principal_hazard_patterns(
        climate_data, 
        n_patterns=5
    )
    
    for hazard, patterns in hazard_patterns.items():
        explained_var = patterns['explained_variance_ratio']
        cumulative_var = patterns['cumulative_variance']
        print(f"ğŸŒŠ {hazard.upper()}: First 3 components explain {cumulative_var[2]:.1%} of variance")
    
    print("\n" + "="*50)
    print("ğŸ—ï¸ DEMO 2: Adaptive Vulnerability Curves")
    print("="*50)
    
    vuln_curves = VulnerabilityCurves()
    
    asset_types = [
        {
            'name': 'Modern Reinforced Building',
            'characteristics': {
                'construction_type': 'reinforced',
                'age': 10,
                'adaptation_score': 0.8
            }
        },
        {
            'name': 'Legacy Vulnerable Building', 
            'characteristics': {
                'construction_type': 'vulnerable',
                'age': 50,
                'adaptation_score': 0.2
            }
        }
    ]
    
    print("ğŸ¢ Creating adaptive vulnerability curves for different asset types...")
    
    for asset_type in asset_types:
        print(f"\nğŸ“ˆ {asset_type['name']}:")
        
        vuln_func = vuln_curves.create_adaptive_vulnerability_curve(
            asset_type['characteristics'],
            'flood'
        )
        
        hazard_intensities = np.linspace(0, 1, 11)
        damage_ratios, uncertainties = vuln_func(hazard_intensities)
        
        print(f"   ğŸŒŠ Flood vulnerability at intensity 0.5: {damage_ratios[5]:.2f} Â± {uncertainties[5]:.2f}")
        print(f"   ğŸŒŠ Flood vulnerability at intensity 1.0: {damage_ratios[10]:.2f} Â± {uncertainties[10]:.2f}")
    
    print("\n" + "="*50)
    print("ğŸ’¼ DEMO 3: Advanced Portfolio Risk Assessment")
    print("="*50)
    
    assets = data_ingestion._generate_synthetic_asset_data(n_assets=100)
    print(f"ğŸ¢ Generated portfolio: {len(assets)} assets, ${assets['value'].sum():,.0f}M total value")
    
    reduced_data = dim_reducer.reduce_dimensions(
        climate_data,
        method='pca',
        n_components=0.95
    )
    
    risk_estimator = RiskEstimation()
    print("âš¡ Estimating portfolio risks with uncertainty quantification...")
    
    risk_estimates = risk_estimator.estimate_portfolio_risk(
        reduced_data,
        assets,
        scenarios=['rcp26', 'rcp45', 'rcp85'],
        time_horizon=30
    )
    
    scenarios = ['rcp26', 'rcp45', 'rcp85']
    print("\nğŸ“Š PORTFOLIO RISK SUMMARY:")
    print("-" * 40)
    
    for scenario in scenarios:
        expected_loss_col = f'expected_loss_{scenario}'
        var_95_col = f'var_95_{scenario}'
        
        if expected_loss_col in risk_estimates.columns:
            total_loss = risk_estimates[expected_loss_col].sum()
            assets_at_risk = (risk_estimates[expected_loss_col] > 0).sum()
            portfolio_var_95 = risk_estimates[var_95_col].sum()
            
            print(f"ğŸŒ¡ï¸ {scenario.upper()}:")
            print(f"   ğŸ’¸ Expected Loss: ${total_loss:,.0f}")
            print(f"   ğŸ¯ Assets at Risk: {assets_at_risk}/{len(assets)} ({assets_at_risk/len(assets)*100:.1f}%)")
            print(f"   ğŸ“ˆ VaR (95%): ${portfolio_var_95:,.0f}")
    
    print("\n" + "="*50)
    print("ğŸ—ºï¸ DEMO 4: Advanced Geospatial Risk Analysis")
    print("="*50)
    
    spatial_analyzer = SpatialAnalyzer()
    
    output_dir = Path("../outputs/demo_results")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    print("ğŸŒ Performing comprehensive spatial analysis...")
    spatial_results = spatial_analyzer.analyze_spatial_risk(
        risk_estimates,
        assets,
        output_dir=str(output_dir)
    )
    
    if 'hotspots' in spatial_results:
        hotspots = spatial_results['hotspots']
        risk_counts = hotspots['risk_counts']
        
        print("\nğŸ”¥ Risk Hotspot Analysis:")
        for risk_level, count in risk_counts.items():
            percentage = count / len(assets) * 100
            print(f"   {risk_level} Risk: {count} assets ({percentage:.1f}%)")
    
    if 'clusters' in spatial_results and spatial_results['clusters']:
        cluster_stats = spatial_results['clusters']['cluster_statistics']
        print(f"\nğŸ¯ Identified {len(cluster_stats)} spatial risk clusters")
        
        for cluster_id, stats in cluster_stats.items():
            print(f"   Cluster {cluster_id}: {stats['n_assets']} assets, ${stats['total_risk']:,.0f} total risk")
    
    print("\n" + "="*50)
    print("ğŸ“‹ DEMO 5: Regulatory Compliance Reports")
    print("="*50)
    
    print("ğŸ“„ Generating TCFD and IFRS S2 compliance reports...")
    
    tcfd_report = risk_estimator.generate_tcfd_report(
        risk_estimates,
        output_path=str(output_dir / "tcfd_report.md")
    )
    
    ifrs_report = risk_estimator.generate_ifrs_s2_report(
        risk_estimates,
        output_path=str(output_dir / "ifrs_s2_report.md")
    )
    
    print(f"âœ… TCFD Report: {tcfd_report}")
    print(f"âœ… IFRS S2 Report: {ifrs_report}")
    
    validation_report = DataValidator.generate_validation_report(
        climate_data, assets, risk_estimates
    )
    
    with open(output_dir / "validation_report.md", 'w') as f:
        f.write(validation_report)
    
    print(f"âœ… Validation Report: {output_dir / 'validation_report.md'}")
    
    print("\n" + "="*60)
    print("ğŸš€ GROUNDBREAKING INNOVATIONS DEMONSTRATED")
    print("="*60)
    
    print("""
ğŸ”¬ TENSOR DECOMPOSITION FOR MULTI-HAZARD COUPLING:
   âœ… Tucker and PARAFAC decomposition on (hazard Ã— location Ã— time) tensors
   âœ… Captures latent interactions between multiple climate hazards
   âœ… Reveals hidden risk patterns traditional methods miss
   
ğŸ—ï¸ ADAPTIVE VULNERABILITY CURVES:
   âœ… Asset-specific functions that adapt based on construction, age, adaptation
   âœ… Bayesian updating with historical loss data
   âœ… Multi-hazard vulnerability interactions
   âœ… Uncertainty quantification for all damage estimates
   
âš¡ ADVANCED RISK ESTIMATION:
   âœ… Bayesian loss models with full uncertainty propagation
   âœ… Monte Carlo simulation with scenario analysis
   âœ… Value-at-Risk and Conditional VaR calculations
   âœ… Regulatory-aligned reporting (TCFD, IFRS S2)
   
ğŸ—ºï¸ GEOSPATIAL RISK NETWORKS:
   âœ… Spatial clustering and hotspot identification
   âœ… Risk corridor analysis for geographic concentrations
   âœ… Interactive mapping with multiple scenario layers
   âœ… Foundation for network-based systemic risk modeling
   
ğŸ”§ PORTABLE MODULAR ARCHITECTURE:
   âœ… Fully modular design for easy technology transfer
   âœ… Comprehensive configuration management
   âœ… Advanced data validation and quality control
   âœ… Extensible framework for novel research integration
    """)
    
    print("\nğŸ¯ NEXT-LEVEL INNOVATIONS READY FOR IMPLEMENTATION:")
    print("""
ğŸŒ Real-Time Tensor Decomposition:
   - Streaming tensor SVD for live climate data updates
   - Online learning for evolving risk patterns
   
ğŸ§  Network-Based Systemic Risk:
   - Graph neural networks for risk contagion modeling
   - Multi-layer network analysis (economic + physical + social)
   
ğŸ“Š Causal Climate Attribution:
   - Identify causal relationships between climate variables and losses
   - Counterfactual analysis for "what-if" scenarios
    """)
    
    print(f"\nâœ… Demo completed successfully!")
    print(f"ğŸ“ All outputs saved to: {output_dir}")
    print(f"ğŸŒ ClimFinRisk platform ready for groundbreaking climate risk analysis!")


if __name__ == "__main__":
    main()
